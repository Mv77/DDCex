{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a Dynamic Discrete Choice Problem with Three Different Methods\n",
    "\n",
    "# Mateo VelÃ¡squez-Giraldo\n",
    "\n",
    "This notebook solves a simple dynamic \"machine replacement\" problem using three different methods:\n",
    "- Contraction mapping iteration.\n",
    "- Hotz-Miller inversion.\n",
    "- Forward simulation.\n",
    "\n",
    "The code is optimized for clarity, not speed, as the purpose is to give a sense of how the three methods work and how they can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, Bounds\n",
    "import pandas as pd\n",
    "import numpy.random as rnd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setup\n",
    "\n",
    "The problem is taken from a problem set of Prof. Nicholas Papageorge Topics in Microeconometrics course at Johns Hopkins University and borrows heavily from Professor Juan Pantano's course Microeconometrics offered at Washington University in St. Louis.\n",
    "\n",
    "There is a shop that operates using a machine. The machine's mantainance costs increase with its age, denoted $a_t$. On each period, the shop must decide whether to replace the machine ($i_t = 1$) or not ($i_t=0$). Assume that costs stop increasing after the machine reaches $a_t = 5$ so that, in practice, that is the maximum age. Age then evolves according to:\n",
    "\\begin{equation}\n",
    "a_{t+1} = \\begin{cases}\n",
    "\t\t\\min \\{5,a_t+1\\}, & \\text{ if } i_t = 0 \\\\\n",
    "\t\t1, & \\text{ if } i_t = 1\n",
    "\t\t\\end{cases}.\n",
    "\\end{equation}\n",
    "A period's profits depend on mantainance costs, replacement costs, and factors that the econometrician does not observe, modeled as stochastic shocks $\\epsilon$:\n",
    "\\begin{equation}\n",
    "    \\Pi (a_t,i_t,\\epsilon_{0,t},\\epsilon_{1,t}) = \\begin{cases}\n",
    "        \\theta a_t + \\epsilon_{0,t} & \\text{if } i_t=0\\\\\n",
    "        R + \\epsilon_{1,t} &  \\text{if } i_t = 1  \n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The shop's problem can be recursively defined as:\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "\t\tV(a_t,\\epsilon_{0,t},\\epsilon_{1,t}) &= \\max_{i_t} \\Pi \n",
    "\t\t(a_t,i_t,\\epsilon_{0,t},\\epsilon_{1,t}) + \\beta \n",
    "\t\tE_t[V(a_{t+1},\\epsilon_{0,t+1},\\epsilon_{1,t+1})]\\\\\n",
    "\t\t&\\text{s.t} \\\\\n",
    "\t\ta_{t+1} &= \\begin{cases}\n",
    "\t\t\\min \\{5,a_t+1\\}, & \\text{ if } i_t = 0 \\\\\n",
    "\t\t1, & \\text{ if } i_t = 1\n",
    "\t\t\\end{cases}.\n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Choice-specific net-of-error value functions\n",
    "$\\bar{V}_0(\\cdot)$ and $\\bar{V}_1(\\cdot)$ can be defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "\t\\bar{V}_0(a_t) =& \\theta_1 a_t + \\\\\n",
    "\t&\\beta E \\left[\\max \\left\\{ \\bar{V}_0\\left( \n",
    "\t\\min \n",
    "\t\\left\\{ 5, a_t \n",
    "\t+1 \\right\\} \\right) + \\epsilon_{0,t+1}, \\bar{V}_1\\left( \\min \n",
    "\t\\left\\{ 5, a_t \n",
    "\t+1 \\right\\} \\right) + \n",
    "\t\\epsilon_{1,t+1} \\right\\} \\right]\n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{V}_1(a_t) = R + \\beta E \\left[\\max \\left\\{ \\bar{V}_0\\left( \n",
    "1 \\right) + \\epsilon_{0,t+1}, \\bar{V}_1\\left(1\\right) + \n",
    "\\epsilon_{1,t+1} \\right\\} \\right].\n",
    "\\end{equation}\n",
    "\n",
    "This allows us to re-express the full value function as\n",
    "\n",
    "\\begin{equation}\n",
    "\tV(a_t,\\epsilon_{0,t},\\epsilon_{1,t}) = \\max \\left\\{ \\bar{V}_0\\left( \n",
    "\ta_t \\right) + \\epsilon_{0,t}, \\bar{V}_1\\left( \n",
    "\ta_t \\right) + \\epsilon_{1,t} \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(i=1|a,\\theta) = \\frac{\\exp (\\bar{V}_1(a_t, \\theta, R))}{\\exp (\\bar{V}_0(a_t, \\theta, R))+\\exp (\\bar{V}_1(a_t, \\theta, R))}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\alpha,\\theta_A, \\theta_B,R) = \\Pi_{j=1}^N \\left[ \\alpha \\times  P\\left( i_j|a_j,\\theta_A, R\\right) + \\left(1-\\alpha \\right) \\times P\\left( i_j|a_j,\\theta_B, R\\right) \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Profit function (the deterministic part)\n",
    "def profit_det(a,i,theta,R):\n",
    "    \n",
    "    if i == 0:\n",
    "        return(theta*a)\n",
    "    else:\n",
    "        return(R)\n",
    "\n",
    "# State transition function\n",
    "def transition(a, i):\n",
    "    if i == 0:\n",
    "        return(min(5,a+1))\n",
    "    else:\n",
    "        return(1)\n",
    "\n",
    "# Compute the log-likelihood of (a,i) vectors given value functions\n",
    "def logL(a, i, V):\n",
    "    \n",
    "    # Compute the probability of each (a,i) pair possible\n",
    "    probs = np.exp(V)\n",
    "    total = np.sum(probs, axis = 1)\n",
    "    probs = probs / total[:,None]\n",
    "    \n",
    "    # Get a vector of the probabilities of observations\n",
    "    L = probs[a-1,i]\n",
    "    logLik = np.sum(np.log(L))\n",
    "    return(logLik)\n",
    "\n",
    "# Construct state and choice vectors\n",
    "states = np.arange(5) + 1\n",
    "choices = np.arange(2)\n",
    "\n",
    "# Construct transition matrices\n",
    "trans_mat = np.zeros((len(choices),len(states),len(states)))\n",
    "# If no-replacement, deterministically move to the next state, up to the last\n",
    "for k in range(len(states)-1):\n",
    "    trans_mat[0][k][k+1] = 1\n",
    "trans_mat[0,len(states)-1,len(states)-1] = 1\n",
    "# If replacement, deterministically move to the first state\n",
    "for k in range(len(states)):\n",
    "    trans_mat[1,k,0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution of the dynamic problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of E[max{V_0 + e0, V_1 + e1}]\n",
    "def expectedMax(V0,V1):\n",
    "    return( np.euler_gamma + np.log( np.exp(V0) + np.exp(V1) ) )\n",
    "\n",
    "# Contraction mapping\n",
    "def contrMapping(Vb, theta, R, beta):\n",
    "    \n",
    "    # Initialize array (rows are a, cols are i)\n",
    "    Vb_1 = np.zeros( Vb.shape )\n",
    "    \n",
    "    for a_ind in range(len(Vb)):\n",
    "        \n",
    "        # Adjust 0 indexing\n",
    "        a = a_ind + 1\n",
    "        \n",
    "        for i in range(2):\n",
    "            \n",
    "            a_1 = transition(a, i)\n",
    "            a_1_ind = a_1 - 1\n",
    "            Vb_1[a_ind, i] = profit_det(a, i, theta, R) + \\\n",
    "                            beta * expectedMax(Vb[a_1_ind,0],Vb[a_1_ind,1])\n",
    "        \n",
    "    return(Vb_1)\n",
    "\n",
    "# Solution of the fixed point problem by repeated application of the\n",
    "# contraction mapping\n",
    "def findFX(V0, theta, R, beta, tol, disp = True):\n",
    "    \n",
    "    V1 = V0\n",
    "    norm = tol + 1\n",
    "    count = 0\n",
    "    while norm > tol:\n",
    "        count = count + 1\n",
    "        V1 = contrMapping(V0, theta, R, beta)\n",
    "        norm = np.linalg.norm(V1 - V0)\n",
    "        if disp:\n",
    "            print('Iter. %i --- Norm of difference is %.6f' % (count,norm))\n",
    "        V0 = V1\n",
    "        \n",
    "    return(V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_dataset(theta, R, nmachines, n_per_machine, beta):\n",
    "\n",
    "    # First solve the choice specific value functions for both parameter sets\n",
    "    V0 = np.zeros((5,2))\n",
    "    tol = 1e-6 # Tolerance\n",
    "    \n",
    "    V = findFX(V0, theta, R, beta, tol, disp = False)\n",
    "    \n",
    "    data = pd.DataFrame(np.zeros((nmachines*n_per_machine,4)),\n",
    "                        columns = ['Id','T','a','i'])\n",
    "    \n",
    "    ind = 0\n",
    "    for m in range(nmachines):\n",
    "        \n",
    "        # Initialize state\n",
    "        a_next = rnd.randint(5) + 1\n",
    "        \n",
    "        for t in range(n_per_machine):\n",
    "            \n",
    "            a = a_next\n",
    "            \n",
    "            # Assign id and time\n",
    "            data.loc[ind,'Id'] = m\n",
    "            data.loc[ind, 'T'] = t\n",
    "            \n",
    "            data.loc[ind, 'a'] = a\n",
    "            \n",
    "            u_replace = V[a - 1][1] + rnd.gumbel()\n",
    "            u_not     = V[a - 1][0] + rnd.gumbel()\n",
    "            \n",
    "            if u_replace < u_not:\n",
    "                data.loc[ind,'i'] = 0\n",
    "                a_next = min(5, a+1)\n",
    "            else:\n",
    "                data.loc[ind,'i'] = 1\n",
    "                a_next = 1\n",
    "                \n",
    "            ind = ind + 1\n",
    "            \n",
    "    return(data)\n",
    "\n",
    "def get_ccps(states, choices):\n",
    "    # Function to estimate ccps. Since we are in a discrete setting,\n",
    "    # these are just frequencies.\n",
    "    \n",
    "    # Find unique states\n",
    "    un_states = np.unique(states)\n",
    "    un_states.sort()\n",
    "    un_choices = np.unique(choices)\n",
    "    un_choices.sort()\n",
    "    \n",
    "    # Initialize ccp matrix\n",
    "    ccps = np.ndarray((len(un_states),len(un_choices)), dtype = float)\n",
    "    \n",
    "    # Fill out the matrix\n",
    "    for i in range(len(un_states)):\n",
    "        \n",
    "        sc = choices[states == un_states[i]]\n",
    "        nobs = len(sc)\n",
    "        \n",
    "        for j in range(len(un_choices)):\n",
    "            \n",
    "            ccps[i][j] = np.count_nonzero( sc == un_choices[j]) / nobs\n",
    "    \n",
    "    return(ccps)\n",
    "\n",
    "def state_state_mat(CPP,transition_mat):\n",
    "    \n",
    "    nstates = CPP.shape[0]\n",
    "    nchoices = CPP.shape[1]\n",
    "    # Initialize\n",
    "    PF = np.zeros((nstates,nstates))\n",
    "    \n",
    "    for i in range(nstates):\n",
    "        for j in range(nstates):\n",
    "            for d in range(nchoices):\n",
    "                PF[i,j] = PF[i,j] + CPP[i,d]*transition_mat[d][i,j]\n",
    "\n",
    "    return(PF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a dataset of a single type\n",
    "nmachines = 6000\n",
    "n_per_machine = 1\n",
    "\n",
    "# Assign test parameters\n",
    "theta = -1\n",
    "R = -4\n",
    "beta = 0.85\n",
    "\n",
    "data = sim_dataset(theta, R, nmachines, n_per_machine, beta)\n",
    "a = data.a.values.astype(int)\n",
    "i = data.i.values.astype(int)\n",
    "\n",
    "# Estimate CPPS\n",
    "cpps = get_ccps(a,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rust's contraction mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Compute the log-likelihood of (a,i) vectors given parameter values,\n",
    "# with contraction mapping method    \n",
    "def logL_par_fx(par, a, i, tol):\n",
    "    \n",
    "    # Extract parameters\n",
    "    theta = par[0]\n",
    "    R = par[1]\n",
    "    beta = par[2]\n",
    "    \n",
    "    # Find implied value functions\n",
    "    V = np.zeros((5,2))\n",
    "    V = findFX(V, theta, R, beta, tol, disp = False)\n",
    "    \n",
    "    # Return the loglikelihood from the implied value function\n",
    "    return(logL(a, i, V) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2783.589762\n",
      "         Iterations: 15\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 19\n",
      "Estimation results (S.E's in parentheses):\n",
      "Theta: -0.9949 (0.0003)\n",
      "R: -3.9996 (0.0041)\n"
     ]
    }
   ],
   "source": [
    "# Set up the objective function for minimization\n",
    "tol  = 1e-9\n",
    "x0   = np.array([0,0]) \n",
    "obj_fun_fx = lambda x: -1 * logL_par_fx([x[0],x[1],beta], a, i, tol)\n",
    "\n",
    "# Optimize\n",
    "est_fx = minimize(obj_fun_fx, x0, method='BFGS', options={'disp': True})\n",
    "mean_est_fx = est_fx.x\n",
    "\n",
    "se_est_fx = np.diag(est_fx.hess_inv)\n",
    "\n",
    "# Present results\n",
    "print('Estimation results (S.E\\'s in parentheses):')\n",
    "print('Theta: %.4f (%.4f)' % (mean_est_fx[0], se_est_fx[0]))\n",
    "print('R: %.4f (%.4f)' % (mean_est_fx[1], se_est_fx[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hotz-Miller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Compute the log-likelihood of (a,i) vectors given parameter values,\n",
    "# with forward simulation method\n",
    "def logL_par_HM(par, a, i,\n",
    "                states, choices, CPPS, trans_mat, \n",
    "                invB):\n",
    "    \n",
    "    # Extract parameters\n",
    "    theta = par[0]\n",
    "    R = par[1]\n",
    "    \n",
    "    # Find implied value functions\n",
    "    V = Hotz_Miller(theta, R, states, choices, CPPS, trans_mat,invB)\n",
    "    \n",
    "    # Return the loglikelihood from the implied value function\n",
    "    return(logL(a, i, V) )\n",
    "\n",
    "\n",
    "\n",
    "def Hotz_Miller(theta, R, states, choices, CPPS, trans_mat,invB):\n",
    "    \n",
    "    nstates = len(states)\n",
    "    nchoices = len(choices)\n",
    "    \n",
    "    # Construct ZE matrix\n",
    "    ZE = np.zeros((nstates, nchoices))\n",
    "    for i in range(nstates):\n",
    "        for j in range(nchoices):\n",
    "            ZE[i,j] = CPPS[i,j]*( profit_det(states[i],choices[j],theta,R) +\n",
    "                                  np.euler_gamma - np.log(CPPS[i,j]) )\n",
    "    \n",
    "    # Take a sum. Is this a typo by Nick?\n",
    "    ZE = np.sum(ZE,1,keepdims = True)\n",
    "    # Compute W\n",
    "    W = np.matmul(invB, ZE)\n",
    "    \n",
    "    # Z and V\n",
    "    Z = np.zeros((nstates,nchoices))\n",
    "    V = np.zeros((nstates,nchoices))\n",
    "    for i in range(nstates):\n",
    "        for j in range(nchoices):\n",
    "            Z[i,j] = np.dot(trans_mat[j][i,:],W)\n",
    "            \n",
    "            V[i,j] = profit_det(states[i],choices[j],theta,R) + beta*Z[i,j]\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2783.589891\n",
      "         Iterations: 12\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 14\n",
      "Estimation results (S.E's in parentheses):\n",
      "Theta: -0.9949 (0.0006)\n",
      "R: -3.9994 (0.0116)\n"
     ]
    }
   ],
   "source": [
    "# Compute the state-to-state (no choice matrix)\n",
    "PF = state_state_mat(cpps,trans_mat)\n",
    "\n",
    "# And the \"inv B\" matrix\n",
    "invB = np.linalg.inv( np.identity(len(states)) - beta*PF )\n",
    "\n",
    "# Set up objective function\n",
    "obj_fun_HM = lambda x: -1 * logL_par_HM(x, a, i,states, choices,\n",
    "                                        cpps, trans_mat, invB)\n",
    "\n",
    "# Optimize\n",
    "est_HM = minimize(obj_fun_HM, x0, method='BFGS', options={'disp': True})\n",
    "mean_est_HM = est_HM.x\n",
    "\n",
    "se_est_HM = np.diag(est_HM.hess_inv)\n",
    "\n",
    "# Present results\n",
    "print('Estimation results (S.E\\'s in parentheses):')\n",
    "print('Theta: %.4f (%.4f)' % (mean_est_HM[0], se_est_HM[0]))\n",
    "print('R: %.4f (%.4f)' % (mean_est_HM[1], se_est_HM[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "Pre-define important structures"
   },
   "source": [
    "## 3. Forward Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "title": "Estimate using contraction mapping"
   },
   "outputs": [],
   "source": [
    "def forward_simul(theta,R,beta,states,choices,CPPS,trans_mat,nperiods,nsims,\n",
    "                  seed):\n",
    "    \n",
    "    # Set seed\n",
    "    rnd.seed(seed)\n",
    "    \n",
    "    # Initialize V\n",
    "    V = np.zeros((len(states),len(choices)))\n",
    "    \n",
    "    for i in range(len(states)):\n",
    "        for j in range(len(choices)):\n",
    "            \n",
    "            v_accum = 0\n",
    "            for r in range(nsims):\n",
    "                \n",
    "                a_ind = i\n",
    "                c_ind = j\n",
    "                v = profit_det(states[a_ind], choices[c_ind], theta, R)\n",
    "                \n",
    "                for t in range(nperiods):\n",
    "                    \n",
    "                    # Simulate state\n",
    "                    a_ind = rnd.choice(a = len(states),\n",
    "                                            p = trans_mat[c_ind][a_ind])\n",
    "                    \n",
    "                    # Simulate choice\n",
    "                    c_ind = rnd.choice(a = len(choices),\n",
    "                                            p = CPPS[a_ind])\n",
    "                    \n",
    "                    # Find expected value of taste disturbance conditional on\n",
    "                    # choice\n",
    "                    exp_e = np.euler_gamma - np.log(CPPS[a_ind,c_ind])\n",
    "                    # Update value funct\n",
    "                    v = v + ( beta**(t+1) ) * (profit_det(states[a_ind],\n",
    "                                                          choices[c_ind],\n",
    "                                                          theta,R) +\n",
    "                                               exp_e)\n",
    "\n",
    "                v_accum = v_accum + v\n",
    "            V[i,j] = v_accum / nsims\n",
    "    \n",
    "    return(V)\n",
    "\n",
    "# Compute the log-likelihood of (a,i) vectors given parameter values,\n",
    "# with forward simulation method\n",
    "def logL_par_fs(par, a, i,\n",
    "                states, choices, CPPS, trans_mat, \n",
    "                nperiods, nsims, seed):\n",
    "    \n",
    "    # Extract parameters\n",
    "    theta = par[0]\n",
    "    R = par[1]\n",
    "    beta = par[2]\n",
    "    \n",
    "    # Find implied value functions\n",
    "    V = forward_simul(theta,R,beta,\n",
    "                      states,choices,\n",
    "                      CPPS,trans_mat,\n",
    "                      nperiods,nsims,\n",
    "                      seed)\n",
    "    # Return the loglikelihood from the implied value function\n",
    "    return(logL(a, i, V) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "title": "Estimate using Hotz-Miller"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2783.639906\n",
      "         Iterations: 10\n",
      "         Function evaluations: 256\n",
      "         Gradient evaluations: 61\n",
      "Estimation results (S.E's in parentheses):\n",
      "Theta: -0.9990 (0.0011)\n",
      "R: -4.0503 (0.0331)\n"
     ]
    }
   ],
   "source": [
    "nperiods = 40\n",
    "nsims    = 30\n",
    "seed     = 1\n",
    "\n",
    "# Set up objective function\n",
    "obj_fun_fs = lambda x: -1 * logL_par_fs([x[0],x[1],beta],a,i,\n",
    "                                        states, choices, cpps, trans_mat,\n",
    "                                        nperiods = nperiods, nsims = nsims,\n",
    "                                        seed = seed)\n",
    "\n",
    "# Optimize\n",
    "est_fs = minimize(obj_fun_fs, x0, method='BFGS', options={'disp': True})\n",
    "mean_est_fs = est_fs.x\n",
    "\n",
    "se_est_fs = np.diag(est_fs.hess_inv)\n",
    "\n",
    "# Present results\n",
    "print('Estimation results (S.E\\'s in parentheses):')\n",
    "print('Theta: %.4f (%.4f)' % (mean_est_fs[0], se_est_fs[0]))\n",
    "print('R: %.4f (%.4f)' % (mean_est_fs[1], se_est_fs[1]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
